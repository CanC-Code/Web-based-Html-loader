<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Hybrid AI Video Processor — Feedback & Regenerate</title>
<link rel="stylesheet" href="style.css" />
<!-- MediaPipe Tasks Vision (used for segmentation) is loaded from CDN inside the module script -->
</head>
<body>
<header>
  <h1>Hybrid AI Video Processor</h1>
  <p class="lead">Automatic person/object cutout with interactive feedback & regeneration. Load a video, press Process, then Like/Dislike and Regenerate to refine. Preview and Download any time.</p>
</header>

<main>
  <section class="mediaBox">
    <div class="mediaInner">
      <input id="videoInput" type="file" accept="video/*">
      <video id="sourceVideo" controls playsinline></video>

      <!-- Canvas shows the live processed frame during processing and also final display while recording -->
      <canvas id="processingCanvas" aria-label="Processed output canvas"></canvas>

      <!-- Replayable preview (hidden until processed blob ready) -->
      <video id="previewVideo" controls style="display:none; width:100%; max-width:840px; border-radius:10px; background:#000;"></video>
    </div>
  </section>

  <section class="controls">
    <select id="modeSelect" class="aux" title="Processing mode">
      <option value="remove">Background Remove (cutout)</option>
      <option value="blur">Background Blur</option>
    </select>

    <button id="processBtn">Process</button>
    <button id="regenerateBtn" disabled>Regenerate</button>

    <button id="likeBtn" disabled>👍 Like</button>
    <button id="dislikeBtn" disabled>👎 Dislike</button>

    <button id="downloadBtn" disabled>💾 Download</button>
  </section>

  <div class="progressWrap">
    <div class="progress"><div id="progressFill"></div></div>
  </div>

  <div id="status">Load a video to begin.</div>
</main>

<script type="module">
/*
  Hybrid AI Video Processor
  - Uses MediaPipe Tasks Vision ImageSegmenter for segmentation.
  - Accumulates a floating-point mask across frames (accumMask) with swayAlpha.
  - Like/Dislike modify accumMask (reinforce or remove areas).
  - Regenerate re-processes the video using the refined accumMask stabilization.
  - Creates a replayable preview (previewVideo) and allows downloading the final WebM.
*/

import { FilesetResolver, ImageSegmenter } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.17";

const videoInput = document.getElementById('videoInput');
const sourceVideo = document.getElementById('sourceVideo');
const canvas = document.getElementById('processingCanvas');
const ctx = canvas.getContext('2d');
const previewVideo = document.getElementById('previewVideo');

const processBtn = document.getElementById('processBtn');
const regenerateBtn = document.getElementById('regenerateBtn');
const likeBtn = document.getElementById('likeBtn');
const dislikeBtn = document.getElementById('dislikeBtn');
const downloadBtn = document.getElementById('downloadBtn');
const modeSelect = document.getElementById('modeSelect');
const progressFill = document.getElementById('progressFill');
const statusEl = document.getElementById('status');

let segmenter = null;
let accumMask = null;           // Float32Array width*height, values 0..1
let maskWidth = 0, maskHeight = 0;
let swayAlpha = 0.25;           // how quickly accumMask updates with new frame (0..1)
let feedbackInfluence = 0.85;   // how strongly Like/Dislike updates accumMask
let processing = false;
let recordedChunks = [];
let recorder = null;
let processedBlob = null;

// Initialize MediaPipe ImageSegmenter (Tasks API)
async function initSegmenter(){
  status('Loading segmentation model...');
  const filesetResolver = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.17/wasm"
  );
  segmenter = await ImageSegmenter.createFromOptions(filesetResolver, {
    baseOptions: {
      modelAssetPath: "https://storage.googleapis.com/mediapipe-assets/selfie_segmenter_landscape.tflite"
    },
    outputCategoryMask: true
  });
  status('Model ready.');
}

// UI helpers
function status(s){ statusEl.textContent = s; console.log('[video_layer]', s); }
function setProgress(p){ progressFill.style.width = `${Math.max(0, Math.min(100, p))}%`; }

// Load a local video
videoInput.addEventListener('change', (e)=>{
  const file = e.target.files[0];
  if(!file) return;
  sourceVideo.src = URL.createObjectURL(file);
  sourceVideo.load();
  sourceVideo.onloadeddata = () => {
    canvas.width = sourceVideo.videoWidth;
    canvas.height = sourceVideo.videoHeight;
    maskWidth = sourceVideo.videoWidth;
    maskHeight = sourceVideo.videoHeight;
    status('Video loaded — ready to process. Choose a mode and click Process.');
    processBtn.disabled = false;
    previewVideo.style.display = 'none';
    previewVideo.src = '';
    processedBlob = null;
    downloadBtn.disabled = true;
    regenerateBtn.disabled = true;
    likeBtn.disabled = true;
    dislikeBtn.disabled = true;
    setProgress(0);
  };
});

// Utility: reset accumMask to undefined or zeros
function resetAccumMask(){
  accumMask = new Float32Array(maskWidth * maskHeight);
  // initialize to 0 (no foreground) for better exclusion, but quickly updated by first frames
  for(let i=0;i<accumMask.length;i++) accumMask[i] = 0;
}

// Convert Mask object (categoryMask) to Uint8Array alpha map 0..255
async function maskToAlphaArray(categoryMask){
  // categoryMask has getAsUint8Array() method returning RGBA-like values but typically uses single channel alpha
  // We create a canvas and draw it, then read alpha.
  const mc = categoryMask;
  // categoryMask in Tasks API provides a method to convert to ImageData or to get bytes; but we will render to an offscreen canvas.
  const tmp = new OffscreenCanvas(mc.width, mc.height);
  const tctx = tmp.getContext('2d');
  // create ImageData from mc.getAsUint8Array()
  const arr = await mc.getAsUint8Array(); // this returns Uint8Array with values 0..255 per pixel (single byte)
  // create RGBA image where R=G=B=255 and A = arr[p]
  const img = new Uint8ClampedArray(mc.width * mc.height * 4);
  for(let p=0;p<mc.width*mc.height;p++){
    const off = p*4;
    img[off] = 255;
    img[off+1] = 255;
    img[off+2] = 255;
    img[off+3] = arr[p]; // alpha
  }
  const id = new ImageData(img, mc.width, mc.height);
  tctx.putImageData(id, 0, 0);
  // return the alpha array (0..255)
  return arr; // Uint8Array length width*height
}

// Composite using accumMask (Float32 0..1) and current frame
function compositeWithAccum(frameCanvas, accumMaskLocal, mode){
  // frameCanvas is a canvas with current sourceVideo frame drawn full-size
  // We'll draw into main canvas (processingCanvas)
  ctx.clearRect(0,0,canvas.width,canvas.height);

  // If blur mode, draw blurred background first
  if(mode === 'blur'){
    // cheap blur by scaling down and back up
    const tmp = document.createElement('canvas');
    tmp.width = Math.max(2, Math.round(canvas.width / 12));
    tmp.height = Math.max(2, Math.round(canvas.height / 12));
    const tc = tmp.getContext('2d');
    tc.drawImage(frameCanvas, 0, 0, tmp.width, tmp.height);
    ctx.save();
    ctx.filter = 'blur(10px)';
    ctx.drawImage(tmp, 0, 0, canvas.width, canvas.height);
    ctx.restore();

    // Now draw foreground using accumMask as alpha (destination-over style)
    // We'll create mask canvas from accumMaskLocal scaled to canvas size
    const maskCanvas = document.createElement('canvas');
    maskCanvas.width = canvas.width;
    maskCanvas.height = canvas.height;
    const mcx = maskCanvas.getContext('2d');
    const maskImgData = mcx.createImageData(maskWidth, maskHeight);
    for(let i=0;i<maskWidth*maskHeight;i++){
      const a = Math.max(0, Math.min(1, accumMaskLocal[i]));
      const off = i*4;
      maskImgData.data[off] = 255;
      maskImgData.data[off+1] = 255;
      maskImgData.data[off+2] = 255;
      maskImgData.data[off+3] = Math.round(a * 255);
    }
    // draw mask scaled to canvas
    mcx.putImageData(maskImgData, 0, 0);
    // scale onto main canvas
    ctx.globalCompositeOperation = 'destination-over';
    // draw the source's foreground masked by mask: draw mask then draw source with source-in.
    // Simpler: draw source, then use mask as destination-in to keep only foreground (like cutout)
    // So first draw the source onto a temporary canvas and then apply mask.
    const srcTmp = document.createElement('canvas');
    srcTmp.width = canvas.width; srcTmp.height = canvas.height;
    const sctx = srcTmp.getContext('2d');
    sctx.drawImage(frameCanvas, 0, 0, canvas.width, canvas.height);
    // apply mask
    sctx.globalCompositeOperation = 'destination-in';
    sctx.drawImage(maskCanvas, 0, 0, canvas.width, canvas.height);
    // draw the result over the blurred background
    ctx.drawImage(srcTmp, 0, 0, canvas.width, canvas.height);
    ctx.globalCompositeOperation = 'source-over';
  } else {
    // Remove mode: show only foreground on transparent background (or on plain bg)
    const maskCanvas = document.createElement('canvas');
    maskCanvas.width = canvas.width;
    maskCanvas.height = canvas.height;
    const mcx = maskCanvas.getContext('2d');
    const maskImgData = mcx.createImageData(maskWidth, maskHeight);
    for(let i=0;i<maskWidth*maskHeight;i++){
      const a = Math.max(0, Math.min(1, accumMaskLocal[i]));
      const off = i*4;
      maskImgData.data[off] = 255;
      maskImgData.data[off+1] = 255;
      maskImgData.data[off+2] = 255;
      maskImgData.data[off+3] = Math.round(a * 255);
    }
    mcx.putImageData(maskImgData, 0, 0);

    const srcTmp = document.createElement('canvas');
    srcTmp.width = canvas.width; srcTmp.height = canvas.height;
    const sctx = srcTmp.getContext('2d');
    sctx.drawImage(frameCanvas, 0, 0, canvas.width, canvas.height);
    sctx.globalCompositeOperation = 'destination-in';
    sctx.drawImage(maskCanvas, 0, 0, canvas.width, canvas.height);

    // clear main canvas and draw the srcTmp (foreground)
    ctx.clearRect(0,0,canvas.width,canvas.height);
    ctx.drawImage(srcTmp, 0, 0, canvas.width, canvas.height);
  }
}

// Apply feedback mask (Uint8Array alpha values) as like/dislike to accumMask
function applyFeedbackAlpha(alphaArr, kind){
  // alphaArr length = maskWidth*maskHeight (0..255)
  const n = accumMask.length;
  if(alphaArr.length !== n) {
    console.warn('feedback alpha size mismatch', alphaArr.length, n);
    return;
  }
  if(kind === 'like'){
    // reinforce: accum = clamp(accum * (1 - f) + 1 * f) where f = feedbackInfluence
    for(let i=0;i<n;i++){
      const a = alphaArr[i] / 255;
      if(a > 0.2){
        accumMask[i] = Math.min(1, (1 - feedbackInfluence) * accumMask[i] + feedbackInfluence * 1.0);
      }
    }
  } else if(kind === 'dislike'){
    // suppress: accum = accum * (1 - f) where a is mask area
    for(let i=0;i<n;i++){
      const a = alphaArr[i] / 255;
      if(a > 0.2){
        accumMask[i] = Math.max(0, accumMask[i] * (1 - feedbackInfluence));
      }
    }
  }
}

// Process the video: play, capture each frame, segment, update accumMask, composite, and record via MediaRecorder
async function processVideo(useAccumFromPrev = true){
  if(!segmenter) {
    status('Model not loaded yet. Please wait.');
    return;
  }
  if(!sourceVideo.src) {
    status('Load a video first.');
    return;
  }

  // initialize accumMask on first run or if user requested reset
  if(!accumMask || !useAccumFromPrev){
    resetAccumMask();
  }

  // Prepare offscreen canvas to draw current frame and to pass to segmenter
  const srcCanvas = new OffscreenCanvas(maskWidth, maskHeight);
  const srcCtx = srcCanvas.getContext('2d');

  recordedChunks = [];
  // prepare MediaRecorder for the processingCanvas stream
  const stream = canvas.captureStream(30);
  const options = { mimeType: 'video/webm;codecs=vp8' };
  recorder = new MediaRecorder(stream, options);
  recorder.ondataavailable = ev => { if(ev.data && ev.data.size) recordedChunks.push(ev.data); };
  recorder.start(250);

  // make sure preview hidden during processing
  previewVideo.style.display = 'none';
  processedBlob = null;
  downloadBtn.disabled = true;
  regenerateBtn.disabled = true;
  likeBtn.disabled = true; dislikeBtn.disabled = true;

  // ensure video is at start and playing
  sourceVideo.currentTime = 0;
  await sourceVideo.play().catch(()=>{ /* autoplay may be blocked; user can click play */ });

  status('Processing — this may take a while depending on video length & CPU.');
  processing = true;

  // Use ImageCapture when available for better grabFrame, fallback to drawImage per RAF
  let useImageCapture = false;
  try {
    const track = sourceVideo.captureStream ? sourceVideo.captureStream().getVideoTracks()[0] : null;
    if(track && typeof ImageCapture !== 'undefined') useImageCapture = true;
  } catch(e){ useImageCapture = false; }

  // choose frame rate target: attempt to follow source frame rate if present, otherwise 30
  const targetFPS = 30;
  const frameDuration = 1000 / targetFPS;

  // Helper: get current frame as ImageBitmap
  async function grabFrameBitmap(){
    if(useImageCapture){
      try {
        const track = sourceVideo.captureStream().getVideoTracks()[0];
        const ic = new ImageCapture(track);
        const bmp = await ic.grabFrame();
        return bmp;
      } catch(e){
        // fallback
      }
    }
    // fallback: draw video into a temporary canvas and create ImageBitmap
    srcCtx.drawImage(sourceVideo, 0, 0, srcCanvas.width, srcCanvas.height);
    return srcCanvas.transferToImageBitmap();
  }

  // Frame loop: process until video ends
  const totalFrames = Math.max(1, Math.floor(sourceVideo.duration * targetFPS));
  let frameCount = 0;
  const startTime = performance.now();

  while(processing && !sourceVideo.ended){
    const frameBitmap = await grabFrameBitmap();
    // draw bitmap to srcCanvas
    srcCtx.clearRect(0,0,srcCanvas.width,srcCanvas.height);
    srcCtx.drawImage(frameBitmap, 0, 0, srcCanvas.width, srcCanvas.height);

    // segmenter.segment expects ImageSource (canvas or ImageBitmap)
    const segmentationResult = await segmenter.segment(srcCanvas);
    const categoryMask = segmentationResult.categoryMask; // Image (mask) object
    // convert to alpha array
    const alphaArr = await maskToAlphaArray(categoryMask); // Uint8Array length width*height

    // Convert alpha (Uint8) to normalized 0..1 current mask
    const n = maskWidth * maskHeight;
    // Update accumMask with sway: accum = (1 - swayAlpha) * accum + swayAlpha * current
    for(let i=0;i<n;i++){
      const cur = alphaArr[i] / 255;
      accumMask[i] = (1 - swayAlpha) * accumMask[i] + swayAlpha * cur;
    }

    // composite and draw to onscreen canvas
    // draw the current source into a temporary canvas sized to mask dimensions, then composite to main canvas
    const tmpCanvas = document.createElement('canvas');
    tmpCanvas.width = maskWidth; tmpCanvas.height = maskHeight;
    const tmpCtx = tmpCanvas.getContext('2d');
    tmpCtx.drawImage(frameBitmap, 0, 0, tmpCanvas.width, tmpCanvas.height);
    compositeWithAccum(tmpCanvas, accumMask, modeSelect.value);

    // clean up bitmap
    frameBitmap.close();

    frameCount++;
    setProgress((frameCount / totalFrames) * 100);

    // throttle: let browser breathe
    await new Promise(r => setTimeout(r, 0));
    // if sourceVideo is paused (user interaction) wait until it plays again or ended
    if(sourceVideo.paused && !sourceVideo.ended) {
      // wait a bit for user to resume; we will not stall the page
      await new Promise(resolve => {
        const onPlay = () => { sourceVideo.removeEventListener('play', onPlay); resolve(); };
        sourceVideo.addEventListener('play', onPlay);
      });
    }
  }

  // finish up
  processing = false;
  // stop recorder after a short delay to ensure last frames get captured
  await new Promise(r => setTimeout(r, 250));
  if(recorder && recorder.state === 'recording') recorder.stop();

  recorder.onstop = () => {
    processedBlob = new Blob(recordedChunks, { type: 'video/webm' });
    previewVideo.src = URL.createObjectURL(processedBlob);
    previewVideo.style.display = 'block';
    previewVideo.load();
    previewVideo.play().catch(()=>{});
    status('Processing complete — preview ready. You may Like/Dislike, Regenerate, or Download.');
    downloadBtn.disabled = false;
    regenerateBtn.disabled = false;
    likeBtn.disabled = false;
    dislikeBtn.disabled = false;
  };

  const elapsed = (performance.now() - startTime) / 1000;
  console.log(`Processed ${frameCount} frames in ${elapsed.toFixed(2)}s`);
}

// Like / Dislike handlers use last segmentation masks implicitly (they use current accumMask)
likeBtn.addEventListener('click', () => {
  // create a pseudo-mask from current accumMask: areas with accum >= 0.4 considered selected
  if(!accumMask) return;
  const alpha = new Uint8Array(maskWidth * maskHeight);
  for(let i=0;i<alpha.length;i++) alpha[i] = accumMask[i] >= 0.4 ? 255 : 0;
  applyFeedbackAlpha(alpha, 'like');
  status('Liked — feedback applied (reinforced).');
});

dislikeBtn.addEventListener('click', () => {
  if(!accumMask) return;
  const alpha = new Uint8Array(maskWidth * maskHeight);
  for(let i=0;i<alpha.length;i++) alpha[i] = accumMask[i] >= 0.2 ? 255 : 0;
  applyFeedbackAlpha(alpha, 'dislike');
  status('Disliked — feedback applied (suppressed).');
});

// Regenerate: re-run processing but keep accumMask (so feedback is used)
regenerateBtn.addEventListener('click', async () => {
  if(!sourceVideo.src) { status('Load a video first.'); return; }
  status('Regenerating with refined mask...');
  // stop any ongoing playback
  if(!sourceVideo.paused) sourceVideo.pause();
  // reprocess using accumMask
  await processVideo(true);
});

// Download final processed blob
downloadBtn.addEventListener('click', ()=>{
  if(!processedBlob){ status('No processed result available.'); return; }
  const a = document.createElement('a');
  a.href = URL.createObjectURL(processedBlob);
  a.download = `processed_${Date.now()}.webm`;
  a.click();
  status('Downloaded processed video.');
});

// Top-level initialization
(async () => {
  await initSegmenter();
  // disable process until a video is loaded (videoInput handler will enable)
  processBtn.disabled = true;

  processBtn.addEventListener('click', async () => {
    status('Starting processing run — this will process entire video and produce a preview.');
    // reset recorded chunks and accumMask? keep previous accumMask if user wants refinement? We'll preserve accumMask if exists.
    if(!accumMask) resetAccumMask();
    await processVideo(true);
  });
})();

</script>
</body>
</html>