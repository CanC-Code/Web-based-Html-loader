<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Video Layer — Live AI Editing</title>
<style>
:root { --bg:#0d1117; --panel:#0f1720; --accent:#58a6ff; --muted:#8b949e; }
body { margin:0; font-family:Arial,sans-serif; background:var(--bg); color:#c9d1d9; display:flex; flex-direction:column; align-items:center; padding:1rem; }
h1 { color:var(--accent); margin:0 0 1rem 0; font-size:1.2rem; }
#controls { display:flex; gap:0.5rem; flex-wrap:wrap; align-items:center; justify-content:center; margin-bottom:1rem; }
select, input[type=file], button { background:var(--panel); color:inherit; border:1px solid #21262d; padding:8px 12px; border-radius:8px; cursor:pointer; font-weight:600; }
#layout { display:flex; gap:14px; width:100%; max-width:1100px; }
#viewer { position:relative; background:#000; border-radius:10px; overflow:hidden; flex:1; }
video, canvas { width:100%; height:auto; display:block; }
#mainCanvas { position:absolute; top:0; left:0; pointer-events:none; }
#sidebar { width:300px; background:var(--panel); border-radius:10px; padding:12px; color:#c9d1d9; }
.section { margin-bottom:12px; }
label { display:block; font-size:0.85rem; color:var(--muted); margin-bottom:6px; }
#status { margin-top:10px; color:var(--muted); font-size:0.9rem; }
</style>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/deeplab@0.2.2/dist/deeplab.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@ffmpeg/ffmpeg@0.12.8/dist/ffmpeg.min.js"></script>
</head>
<body>
<h1>Video Layer — Live AI Editing</h1>

<div id="controls">
  <input type="file" id="videoInput" accept="video/*">
  <select id="modeSelect">
    <option value="auto">Auto Focus</option>
    <option value="replace">Content Replace</option>
    <option value="transparent">Transparent Background</option>
    <option value="original">Original</option>
  </select>
  <button id="exportBtn" disabled>Export Processed Video</button>
</div>

<div id="layout">
  <div id="viewer">
    <video id="video" playsinline controls></video>
    <canvas id="mainCanvas"></canvas>
  </div>
  <aside id="sidebar">
    <div class="section">
      <label>Status:</label>
      <div id="status">Idle.</div>
    </div>
  </aside>
</div>

<script>
const videoInput = document.getElementById('videoInput');
const video = document.getElementById('video');
const mainCanvas = document.getElementById('mainCanvas');
const mainCtx = mainCanvas.getContext('2d');
const modeSelect = document.getElementById('modeSelect');
const exportBtn = document.getElementById('exportBtn');
const statusEl = document.getElementById('status');

let deeplabModel = null;
let processing = false;
let frames = [];
let fps = 25;

async function setStatus(msg){ statusEl.textContent = msg; console.log('[video_layer]', msg); }

// Auto-load DeepLab
async function loadModel() {
  setStatus('Loading DeepLab model...');
  deeplabModel = await deeplab.load({ base: 'ade20k', quantizationBytes: 2 });
  setStatus('Model loaded: ADE20K');
}
loadModel();

// Load video
videoInput.addEventListener('change', e=>{
  const f = e.target.files[0];
  if(!f) return;
  video.src = URL.createObjectURL(f);
  video.load();
  video.onloadeddata = ()=>{
    mainCanvas.width = video.videoWidth;
    mainCanvas.height = video.videoHeight;
    setStatus(`Video loaded (${video.videoWidth}x${video.videoHeight})`);
    exportBtn.disabled=false;
  };
});

// Draw live composite
async function drawFrame(){
  if(video.paused || video.ended) return;
  const w = mainCanvas.width;
  const h = mainCanvas.height;
  mainCtx.clearRect(0,0,w,h);
  mainCtx.drawImage(video,0,0,w,h);

  if(!deeplabModel) return;

  const off = document.createElement('canvas');
  off.width = 256; off.height = 256;
  const offCtx = off.getContext('2d');
  offCtx.drawImage(video,0,0,256,256);

  const seg = await deeplabModel.segment(off);
  const classMap = seg.segmentationMap;
  const legend = seg.legend;

  const maskCanvas = document.createElement('canvas');
  maskCanvas.width = w; maskCanvas.height = h;
  const mCtx = maskCanvas.getContext('2d');
  const imageData = mCtx.createImageData(w,h);
  const scaleX = w/256, scaleY = h/256;

  for(let y=0;y<256;y++){
    for(let x=0;x<256;x++){
      const idx = y*256+x;
      const cid = classMap[idx];
      const px = Math.floor(x*scaleX);
      const py = Math.floor(y*scaleY);
      const offIdx = (py*w+px)*4;
      const mode = modeSelect.value;
      if(mode==='auto'){
        const name = legend[cid] || '';
        if(name.toLowerCase().includes('person') || name.toLowerCase().includes('animal')){
          imageData.data[offIdx+3]=255;
        } else imageData.data[offIdx+3]=0;
      } else if(mode==='transparent'){
        imageData.data[offIdx+3]=cid>0?255:0;
      } else if(mode==='replace'){
        imageData.data[offIdx+0]=0;
        imageData.data[offIdx+1]=0;
        imageData.data[offIdx+2]=0;
        imageData.data[offIdx+3]=255;
      } else {
        imageData.data[offIdx+3]=255;
      }
    }
  }

  mCtx.putImageData(imageData,0,0);
  if(modeSelect.value!=='original'){
    mainCtx.globalCompositeOperation='destination-in';
    mainCtx.drawImage(maskCanvas,0,0,w,h);
    mainCtx.globalCompositeOperation='source-over';
  }

  requestAnimationFrame(drawFrame);
}

// Start live processing
video.addEventListener('play', ()=>{
  drawFrame();
});

// Export via FFmpeg.js
exportBtn.addEventListener('click', async ()=>{
  if(!video.src) return;
  setStatus('Exporting processed video... this may take a while');

  const { createFFmpeg, fetchFile } = FFmpeg;
  const ffmpeg = createFFmpeg({ log:true });
  await ffmpeg.load();

  const totalFrames = Math.floor(video.duration*fps);
  for(let i=0;i<totalFrames;i++){
    video.currentTime = i/fps;
    await new Promise(r=>video.onseeked=r);
    await drawFrame();
    const blob = await new Promise(r=>mainCanvas.toBlob(r,'image/png'));
    ffmpeg.FS('writeFile',`frame${String(i).padStart(4,'0')}.png`, new Uint8Array(await blob.arrayBuffer()));
    setStatus(`Captured frame ${i+1}/${totalFrames}`);
  }

  await ffmpeg.run('-r',String(fps),'-i','frame%04d.png','-c:v','libvpx','-pix_fmt','yuva420p','out.webm');
  const data = ffmpeg.FS('readFile','out.webm');
  const blob = new Blob([data.buffer],{type:'video/webm'});
  const url = URL.createObjectURL(blob);

  const a = document.createElement('a');
  a.href=url;
  a.download=`processed_${Date.now()}.webm`;
  a.click();
  setStatus('Export complete!');
});
</script>
</body>
</html>
