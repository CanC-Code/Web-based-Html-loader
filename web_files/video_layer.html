<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Hybrid AI Video Processor — Worker + Feedback</title>
<link rel="stylesheet" href="style.css" />
<!-- MediaPipe Tasks Vision -->
<script type="module">
  // Nothing here — module will be imported inside main script below
</script>
</head>
<body>
<header>
  <h1>Hybrid AI Video Processor</h1>
  <p class="lead">Worker-based processing, automatic exclusion (fans), multiple effects, feedback & regenerate. No persistence.</p>
</header>

<main>
  <section class="mediaBox">
    <div class="mediaInner">
      <input id="videoInput" type="file" accept="video/*">
      <video id="sourceVideo" controls playsinline></video>
      <canvas id="displayCanvas" aria-label="Processed output"></canvas>
      <video id="previewVideo" controls style="display:none; width:100%; max-width:840px; border-radius:10px; background:#000;"></video>
    </div>
  </section>

  <section class="controls">
    <select id="effectSelect" class="aux" title="Effect">
      <option value="remove">Remove Background (cutout)</option>
      <option value="blur">Blur Background</option>
      <option value="replaceColor">Replace Background Color</option>
      <option value="desaturate">Desaturate Background</option>
    </select>

    <input id="replaceColor" type="color" value="#0d1117" title="Background color" style="margin-left:6px;">

    <button id="processBtn">Process</button>
    <button id="regenerateBtn" disabled>Regenerate</button>

    <button id="likeBtn" disabled>👍 Like</button>
    <button id="dislikeBtn" disabled>👎 Dislike</button>

    <button id="downloadBtn" disabled>💾 Download</button>
  </section>

  <div class="progressWrap">
    <div class="progress"><div id="progressFill"></div></div>
  </div>

  <div id="status">Load a video to begin.</div>
</main>

<script type="module">
import { FilesetResolver, ImageSegmenter } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.17";

const videoInput = document.getElementById('videoInput');
const sourceVideo = document.getElementById('sourceVideo');
const canvas = document.getElementById('displayCanvas');
const ctx = canvas.getContext('2d');
const previewVideo = document.getElementById('previewVideo');

const processBtn = document.getElementById('processBtn');
const regenerateBtn = document.getElementById('regenerateBtn');
const likeBtn = document.getElementById('likeBtn');
const dislikeBtn = document.getElementById('dislikeBtn');
const downloadBtn = document.getElementById('downloadBtn');
const effectSelect = document.getElementById('effectSelect');
const replaceColorInput = document.getElementById('replaceColor');
const progressFill = document.getElementById('progressFill');
const statusEl = document.getElementById('status');

// worker script filename (must be in same folder)
const WORKER_FILE = 'processor-worker.js';

let segmenter = null;
let worker = null;
let maskWidth = 0, maskHeight = 0;
let processing = false;
let recordedChunks = [];
let recorder = null;
let processedBlob = null;
let lastAlpha = null; // last alpha map Uint8Array
let frameCount = 0;
let totalFramesEstimate = 0;
let targetFPS = 25;

// initialize MediaPipe segmenter
async function initSegmenter(){
  status('Loading segmentation model...');
  const filesetResolver = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.17/wasm"
  );
  segmenter = await ImageSegmenter.createFromOptions(filesetResolver, {
    baseOptions: { modelAssetPath: "https://storage.googleapis.com/mediapipe-assets/selfie_segmenter_landscape.tflite" },
    outputCategoryMask: true
  });
  status('Segmentation model ready.');
}

// init worker
function initWorker(){
  if(worker) worker.terminate();
  worker = new Worker(WORKER_FILE);
  worker.onmessage = e => {
    const d = e.data;
    if(d.type === 'log') {
      console.log('[worker]', d.msg);
      return;
    }
    if(d.type === 'processedBitmap'){
      // received processed ImageBitmap -> draw to canvas
      const bitmap = d.bitmap;
      // draw then close bitmap
      (async ()=> {
        // draw onto canvas
        ctx.clearRect(0,0,canvas.width,canvas.height);
        ctx.drawImage(bitmap, 0, 0, canvas.width, canvas.height);
        bitmap.close();
      })();
      // update progress if provided
      if(typeof d.progress === 'number') setProgress(d.progress);
      if(d.final === true){
        // finalize: worker has finished processing run and posted final flag and may send blob via transferable? we record locally via MediaRecorder
        status('Processing complete — preview ready.');
        // stop recording after small delay
        if(recorder && recorder.state === 'recording'){ setTimeout(()=> recorder.stop(), 300); }
        regenerateBtn.disabled = false;
        likeBtn.disabled = false;
        dislikeBtn.disabled = false;
        downloadBtn.disabled = false;
      }
    }
    if(d.type === 'debug') console.log('[worker debug]', d.obj);
  };
}

// UI helpers
function status(s){ statusEl.textContent = s; console.log('[ui]', s); }
function setProgress(p){ progressFill.style.width = `${Math.max(0, Math.min(100, p))}%`; }

// on video select
videoInput.addEventListener('change', e => {
  const file = e.target.files[0];
  if(!file) return;
  sourceVideo.src = URL.createObjectURL(file);
  sourceVideo.load();
  sourceVideo.onloadeddata = () => {
    canvas.width = sourceVideo.videoWidth;
    canvas.height = sourceVideo.videoHeight;
    maskWidth = sourceVideo.videoWidth;
    maskHeight = sourceVideo.videoHeight;
    // estimate frames
    targetFPS = Math.min(30, Math.max(15, Math.round(1 / (sourceVideo.webkitDecodedFrameCount ? (1/30) : 1/25))));
    totalFramesEstimate = Math.ceil((sourceVideo.duration || 0) * targetFPS);
    status('Video loaded. Click Process to start. Model must be ready.');
    processBtn.disabled = false;
    previewVideo.style.display = 'none';
    previewVideo.src = '';
    processedBlob = null;
    downloadBtn.disabled = true;
    regenerateBtn.disabled = true;
    likeBtn.disabled = true;
    dislikeBtn.disabled = true;
    setProgress(0);
  };
});

// Prepare worker and recorder, then start processing loop (main thread segments each frame)
// We send each raw frame ImageBitmap + alphaArray to worker for accumulation and effects
async function startProcessing(){
  if(!segmenter) { status('Model not loaded yet'); return; }
  if(!sourceVideo.src) { status('Load a video first'); return; }

  initWorker();

  // Start MediaRecorder on canvas stream to capture final processed video
  recordedChunks = [];
  const stream = canvas.captureStream(targetFPS);
  try {
    recorder = new MediaRecorder(stream, { mimeType: 'video/webm;codecs=vp8' });
  } catch(e) {
    // fallback without codec specified
    recorder = new MediaRecorder(stream);
  }
  recorder.ondataavailable = ev => { if(ev.data && ev.data.size) recordedChunks.push(ev.data); };
  recorder.onstop = () => {
    processedBlob = new Blob(recordedChunks, { type: 'video/webm' });
    previewVideo.src = URL.createObjectURL(processedBlob);
    previewVideo.style.display = 'block';
    previewVideo.load();
    previewVideo.play().catch(()=>{});
    status('Preview ready — you can replay and download.');
  };
  recorder.start(250);

  // begin processing run
  processing = true;
  frameCount = 0;
  setProgress(0);
  status('Processing started — running segmentation then worker processing.');

  // ensure worker initialized
  await new Promise(r => setTimeout(r, 50));

  // frame-capture loop: we'll read video frames as ImageBitmap using drawImage approach
  const off = new OffscreenCanvas(maskWidth, maskHeight);
  const offCtx = off.getContext('2d');

  // bring video to start
  sourceVideo.currentTime = 0;
  await sourceVideo.play().catch(()=>{}); // may need user gesture

  // Use a loop driven by requestAnimationFrame; stop when video ends
  async function tick(){
    if(!processing) return;

    // draw current frame to offscreen
    offCtx.drawImage(sourceVideo, 0, 0, off.width, off.height);

    // segmentation on offscreen: Task API accepts canvas as ImageSource
    const result = await segmenter.segment(off); // synchronous-ish promise
    const categoryMask = result.categoryMask;
    const alphaArr = await categoryMask.getAsUint8Array(); // Uint8Array length w*h, values 0..255

    lastAlpha = alphaArr; // store for feedback
    // create ImageBitmap of the current frame to send to worker (transfer)
    const bitmap = await createImageBitmap(off);

    // prepare message
    worker.postMessage({
      type: 'processFrame',
      frameBitmap: bitmap,
      alpha: alphaArr.buffer,
      w: maskWidth,
      h: maskHeight,
      effect: effectSelect.value,
      replaceColor: replaceColorInput.value,
      progress: Math.round((frameCount / Math.max(1, totalFramesEstimate)) * 100)
    }, [bitmap, alphaArr.buffer]);

    frameCount++;
    setProgress(Math.min(100, (frameCount / Math.max(1, totalFramesEstimate)) * 100));

    // advance video playback to next frame - we simply proceed while playing
    if(!sourceVideo.paused && !sourceVideo.ended) {
      // schedule next frame
      requestAnimationFrame(tick);
    } else if(sourceVideo.ended) {
      // tell worker processing of run is finished so it can finalize if needed
      worker.postMessage({ type: 'finalize' });
      processing = false;
    } else {
      // if paused by user, wait for play
      sourceVideo.addEventListener('play', () => {
        requestAnimationFrame(tick);
      }, { once: true });
    }
  }

  requestAnimationFrame(tick);
}

// stop processing (user can regenerate or stop)
function stopProcessing(){
  processing = false;
  status('Processing stopped by user.');
  if(recorder && recorder.state === 'recording') recorder.stop();
}

// like/dislike: send the lastAlpha map (if present) to worker as feedback
likeBtn.addEventListener('click', () => {
  if(!lastAlpha) { status('No frame available to like.'); return; }
  worker.postMessage({ type: 'feedback', kind: 'like', alpha: lastAlpha.buffer }, [lastAlpha.buffer]);
  // lastAlpha.buffer transferred, make lastAlpha undefined
  lastAlpha = null;
  status('Liked this frame — feedback sent to worker.');
});

dislikeBtn.addEventListener('click', () => {
  if(!lastAlpha) { status('No frame available to dislike.'); return; }
  worker.postMessage({ type: 'feedback', kind: 'dislike', alpha: lastAlpha.buffer }, [lastAlpha.buffer]);
  lastAlpha = null;
  status('Disliked this frame — feedback sent to worker.');
});

// regenerate: tell worker to keep accumMask and run again (we re-run processing to create improved result)
regenerateBtn.addEventListener('click', async () => {
  if(!sourceVideo.src) { status('Load a video first.'); return; }
  // stop any current run
  stopProcessing();
  await new Promise(r => setTimeout(r, 200));
  // worker should already have accumMask including feedback; we just re-run startProcessing
  startProcessing();
  status('Regeneration started — using accumulated feedback to refine output.');
});

// download final blob
downloadBtn.addEventListener('click', () => {
  if(!processedBlob) { status('No processed video available.'); return; }
  const a = document.createElement('a');
  a.href = URL.createObjectURL(processedBlob);
  a.download = `processed_${Date.now()}.webm`;
  a.click();
  status('Download started.');
});

// Process button
processBtn.addEventListener('click', async () => {
  // initialize segmenter and worker if required
  if(!segmenter) {
    await initSegmenter();
  }
  initWorker();
  startProcessing();
});

// initialize segmentation model on load to minimize wait when user presses Process
(async ()=> {
  // warm model loading in background
  try {
    await initSegmenter();
  } catch(err){
    console.error('Model load failed', err);
    status('Model load failed — check network / browser support.');
  }
})();

</script>
</body>
</html>