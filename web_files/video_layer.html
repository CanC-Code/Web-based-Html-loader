<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Video Layer — Reliable Segmentation + Export</title>
<style>
  :root{ --bg:#0d1117; --panel:#0f1720; --muted:#8b949e; --accent:#58a6ff; --btn:#238636; }
  body{ margin:0; font-family:Inter,Arial,sans-serif; background:var(--bg); color:#c9d1d9; display:flex; flex-direction:column; align-items:center; padding:12px; }
  h1{ color:var(--accent); margin:8px 0; font-size:1.15rem; }
  .toolbar{ display:flex; gap:8px; flex-wrap:wrap; align-items:center; margin-bottom:8px; width:100%; max-width:1100px; justify-content:center; }
  .control, button, select, input[type=file] { background:var(--panel); color:inherit; border:1px solid #21262d; padding:8px 10px; border-radius:8px; font-weight:600; cursor:pointer; }
  .control:disabled{ opacity:0.5; cursor:not-allowed; }
  #container{ display:grid; grid-template-columns:1fr 320px; gap:12px; width:100%; max-width:1100px; }
  #viewer{ position:relative; background:#000; border-radius:10px; overflow:hidden; }
  video, canvas { width:100%; display:block; max-width:100%; }
  #mainCanvas{ position:absolute; left:0; top:0; pointer-events:none; }
  #sidebar{ background:var(--panel); padding:12px; border-radius:10px; min-height:220px; }
  label{ display:block; font-size:0.85rem; color:var(--muted); margin:6px 0; }
  .small{ font-size:0.9rem; color:var(--muted); }
  .btn-row{ display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
  #status{ margin-top:8px; color:var(--muted); font-size:0.9rem; }
  @media (max-width:920px){ #container{ grid-template-columns:1fr; } #sidebar{ order:2; } }
</style>

<!-- MediaPipe SelfieSegmentation -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/selfie_segmentation.js"></script>
</head>
<body>
  <h1>Video Layer — Segmentation + Export (stable)</h1>

  <div class="toolbar">
    <input id="videoInput" type="file" accept="video/*" class="control" />
    <button id="loadSeg" class="control">Load Segmentation</button>
    <select id="modeSelect" class="control">
      <option value="original">Original</option>
      <option value="person">Keep Person Only</option>
      <option value="blur">Blur Background</option>
      <option value="transparent">Transparent Background</option>
    </select>
    <label class="small">Blur level</label>
    <input id="blurRange" type="range" min="0" max="30" value="12" style="width:140px;">
    <div class="btn-row">
      <button id="toggleSeg" class="control" disabled>Start Live Seg</button>
      <button id="exportBtn" class="control" disabled>Start Export (WebM)</button>
      <button id="downloadBtn" class="control" disabled>Download WebM</button>
    </div>
  </div>

  <div id="container">
    <div id="viewer">
      <video id="video" playsinline controls></video>
      <canvas id="mainCanvas"></canvas>
    </div>

    <div id="sidebar">
      <div><strong>Status</strong></div>
      <div id="status">Segmentation not loaded.</div>
      <div style="margin-top:12px;">
        <label>Export</label>
        <div class="small">Recording uses the processed canvas. Recording will start when you click Start Export and play the video. Recording stops automatically when the video ends.</div>
      </div>
      <div style="margin-top:12px;">
        <label>Preview / Download</label>
        <div class="btn-row">
          <button id="previewBtn" class="control" disabled>Preview WebM</button>
        </div>
      </div>
    </div>
  </div>

<script>
/* Reliable, minimal implementation:
 - SelfieSegmentation via CDN
 - Live segmentation loop using requestVideoFrameCallback
 - Offscreen temp canvas used to create masked person layer (avoids weird composite issues)
 - Export uses MediaRecorder from mainCanvas.captureStream -> WebM
 - Buttons are enabled/disabled properly and user-visible status updates are provided
*/

// Elements
const videoInput = document.getElementById('videoInput');
const loadSegBtn = document.getElementById('loadSeg');
const toggleSegBtn = document.getElementById('toggleSeg');
const exportBtn = document.getElementById('exportBtn');
const downloadBtn = document.getElementById('downloadBtn');
const previewBtn = document.getElementById('previewBtn');
const modeSelect = document.getElementById('modeSelect');
const blurRange = document.getElementById('blurRange');

const video = document.getElementById('video');
const mainCanvas = document.getElementById('mainCanvas');
const mainCtx = mainCanvas.getContext('2d');

const statusEl = document.getElementById('status');

let seg = null;
let segmentationOn = false;
let currentMask = null; // segmentationMask image
let previewLoopToken = null;

// Recording state
let recorder = null;
let recordedChunks = [];
let webmUrl = null;

// Offscreen temp canvas for compositing masks (same size as main canvas)
function createTempCanvas(w,h){ const c=document.createElement('canvas'); c.width=w; c.height=h; return c; }

// Update status helper
function setStatus(s){ statusEl.textContent = s; console.log(s); }

// Load segmentation model
loadSegBtn.addEventListener('click', () => {
  loadSegBtn.disabled = true;
  setStatus('Loading MediaPipe SelfieSegmentation...');
  try {
    seg = new SelfieSegmentation.SelfieSegmentation({ locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/${f}` });
    seg.setOptions({ modelSelection: 1 }); // 0 = general, 1 = landscape? modelSelection 1 is more accurate for person
    seg.onResults(onSegResults);
    setStatus('SelfieSegmentation ready. Load a video then click "Start Live Seg".');
    toggleSegBtn.disabled = false;
  } catch (err) {
    console.error(err);
    setStatus('Failed to initialize segmentation: ' + (err.message || err));
    loadSegBtn.disabled = false;
  }
});

// When segmentation results arrive
function onSegResults(results){
  // results.segmentationMask is either an ImageBitmap or an HTMLImageElement/canvas
  currentMask = results.segmentationMask;
  // we do not draw here directly, drawing occurs in render loop
}

// Video input load
videoInput.addEventListener('change', e => {
  const f = e.target.files[0];
  if(!f) return;
  video.src = URL.createObjectURL(f);
  video.load();
  video.onloadedmetadata = () => {
    mainCanvas.width = video.videoWidth;
    mainCanvas.height = video.videoHeight;
    setStatus(`Video loaded ${video.videoWidth}×${video.videoHeight}`);
    toggleSegBtn.disabled = (seg==null);
    exportBtn.disabled = false;
    previewBtn.disabled = true;
    downloadBtn.disabled = true;
    // start drawing loop (even if segmentation off, show original)
    startRenderLoop();
  };
});

// Start / stop live segmentation
toggleSegBtn.addEventListener('click', () => {
  if(!seg) { setStatus('Load segmentation model first.'); return; }
  segmentationOn = !segmentationOn;
  toggleSegBtn.textContent = segmentationOn ? 'Stop Live Seg' : 'Start Live Seg';
  setStatus(segmentationOn ? 'Live segmentation ON' : 'Live segmentation OFF');
  if(segmentationOn) startSegLoop();
});

// Start segmentation loop: send frames into MediaPipe
function startSegLoop(){
  // The render loop will call seg.send({image: video}) each frame if segmentationOn
  // Ensure video is playing
  if(video.paused) {
    video.play().catch(()=>{ /* autoplay may be blocked; user can press play */ });
  }
}

// Main render loop: always draws composite into mainCanvas
function startRenderLoop(){
  // use requestVideoFrameCallback when available
  function drawFrame(){
    if(video.readyState < 2){ previewLoopToken = requestAnimationFrame(drawFrame); return; }
    const w = mainCanvas.width = video.videoWidth;
    const h = mainCanvas.height = video.videoHeight;

    // If segmentation is enabled, send current video frame to seg
    if(segmentationOn && seg && video.readyState >= 2){
      // send video to segmentation
      try {
        seg.send({ image: video });
      } catch(e){
        // some browsers may throw if sending too quickly, ignore
      }
    }

    // Draw current composite according to mode and currentMask
    drawCompositeFrame(w,h);

    // schedule next
    if (video.requestVideoFrameCallback) {
      try {
        video.requestVideoFrameCallback(() => drawFrame());
      } catch (e) {
        previewLoopToken = requestAnimationFrame(drawFrame);
      }
    } else {
      previewLoopToken = requestAnimationFrame(drawFrame);
    }
  }

  if(!previewLoopToken) drawFrame();
}

// Draw composite frame into mainCanvas
function drawCompositeFrame(w,h){
  const mode = modeSelect.value;
  // Draw base video
  mainCtx.clearRect(0,0,w,h);

  // If no mask yet or segmentation OFF -> just draw video
  if(!currentMask || !segmentationOn){
    mainCtx.drawImage(video,0,0,w,h);
    return;
  }

  // Use a temp canvas to create person cutout
  const tmpPerson = createTempCanvas(w,h);
  const tctx = tmpPerson.getContext('2d');

  // Draw the original video into tctx
  tctx.drawImage(video,0,0,w,h);

  // Apply mask: destination-in keeps pixels where mask exists
  tctx.globalCompositeOperation = 'destination-in';
  // draw mask (currentMask may be smaller - scale to full canvas)
  tctx.drawImage(currentMask, 0, 0, w, h);
  tctx.globalCompositeOperation = 'source-over';

  if(mode === 'person' || mode === 'transparent'){
    // Person-only or transparent: clear main canvas, then draw masked person
    mainCtx.clearRect(0,0,w,h);
    mainCtx.drawImage(tmpPerson,0,0,w,h);
    // if person mode (not transparent), place black background behind (optional)
    if(mode === 'person'){
      // person on black background is already drawn because canvas background is black; leave as-is
    }
    // transparent leaves alpha where background removed
    return;
  }

  if(mode === 'blur'){
    // blur background: draw blurred video as base, then draw sharp person
    // cheap blur: scale down & scale up + canvas filter if available
    // first draw blurred background
    const blurAmount = parseInt(blurRange.value,10) || 12;
    // create small canvas
    const small = createTempCanvas(Math.max(1,Math.round(w/12)), Math.max(1,Math.round(h/12)));
    const sc = small.getContext('2d');
    sc.drawImage(video,0,0,small.width, small.height);
    mainCtx.save();
    // use canvas filter if supported
    mainCtx.filter = `blur(${blurAmount}px)`;
    mainCtx.drawImage(small,0,0,w,h);
    mainCtx.filter = 'none';
    mainCtx.restore();
    // then draw sharp person from tmpPerson on top
    mainCtx.drawImage(tmpPerson,0,0,w,h);
    return;
  }

  // default (original): draw original video; optionally overlay person outline? We'll just draw original
  mainCtx.drawImage(video,0,0,w,h);
}

// Exporting: record mainCanvas captureStream with MediaRecorder
exportBtn.addEventListener('click', async () => {
  if(!video.src){ setStatus('Load a video before exporting.'); return; }
  if(recordedChunks.length) recordedChunks = []; // clear old
  setStatus('Preparing to record. Start playing the video to capture, or press Play.');
  downloadBtn.disabled = true;
  previewBtn.disabled = true;

  const fps = 25;
  const stream = mainCanvas.captureStream(fps);
  recordedChunks = [];
  try {
    recorder = new MediaRecorder(stream, { mimeType: 'video/webm;codecs=vp8' });
  } catch(e){
    recorder = new MediaRecorder(stream);
  }
  recorder.ondataavailable = e => { if(e.data && e.data.size) recordedChunks.push(e.data); };
  recorder.onstop = () => {
    const blob = new Blob(recordedChunks, { type: 'video/webm' });
    if(webmUrl) URL.revokeObjectURL(webmUrl);
    webmUrl = URL.createObjectURL(blob);
    downloadBtn.disabled = false;
    previewBtn.disabled = false;
    setStatus('Recording finished — preview or download available.');
  };

  recorder.start();
  setStatus('Recording... press Play on the video to capture and wait until it ends. Recording stops automatically when the video ends.');

  // ensure we start at 0
  try {
    video.currentTime = 0;
    await video.play();
  } catch(err){
    setStatus('Autoplay blocked — press Play on the video to begin recording.');
  }

  // stop recorder when video ends
  const onEnded = () => {
    if(recorder && recorder.state === 'recording') recorder.stop();
    video.removeEventListener('ended', onEnded);
  };
  video.addEventListener('ended', onEnded);
});

// Download & preview handlers
downloadBtn.addEventListener('click', ()=>{
  if(!webmUrl){ setStatus('Nothing to download.'); return; }
  const a = document.createElement('a');
  a.href = webmUrl;
  a.download = `processed_${Date.now()}.webm`;
  a.click();
});

previewBtn.addEventListener('click', ()=>{
  if(!webmUrl){ setStatus('No preview available.'); return; }
  // open preview in a new window or replace video src temporarily
  const win = window.open('');
  // embed a video element
  win.document.write(`<video controls autoplay style="width:100%"></video>`);
  const v = win.document.querySelector('video');
  v.src = webmUrl;
});

// Ensure render loop starts when page is ready
startRenderLoop();
setStatus('Ready. Load segmentation model and a video, then enable live segmentation and use the modes.');

</script>
</body>
</html>
